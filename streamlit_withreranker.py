import os
import sys
import httpx
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv, find_dotenv
import streamlit as st
from supabase.client import create_client, Client
import json 
from datetime import datetime

# ğŸš¨ COHERE ADDED: Import Cohere Client
try:
    import cohere
except ImportError:
    st.error("âŒ Ø®Ø·Ø§ÛŒ Ù†ØµØ¨: Ù„Ø·ÙØ§Ù‹ 'pip install cohere' Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")
    sys.exit(1)

# ---------------- Streamlit Configuration (MUST BE FIRST COMMAND) ----------------
st.set_page_config(page_title="Cohere RAG Chatbot (Multilingual/1024 Dim)", layout="wide")

# ---------------- ENV & CONFIG ----------------
load_dotenv(find_dotenv(), override=True)

GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
COHERE_API_KEY = os.environ.get("COHERE_API_KEY") 

# ğŸš¨ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Cohere Multilingual Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
EMBED_MODEL = "embed-multilingual-v3.0"  
EMBED_DIM = 1024                         

# ğŸš¨ğŸš¨ Ù…Ø¯Ù„ Ø±ÛŒØ±Ù†Ú©Ø± Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ±
RERANK_MODEL = "rerank-multilingual-v3.0" 
GEMINI_MODEL = "gemini-2.5-flash" 

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

# ğŸš¨ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Reranking
TOP_K_RETRIEVAL = 15 
# ğŸš¨ğŸš¨ ØªÙ†Ø¸ÛŒÙ… Ù†Ù‡Ø§ÛŒÛŒ: 5 Ú†Ø§Ù†Ú© Ù…Ø±ØªØ¨Ø·â€ŒØªØ±ÛŒÙ† (Ø¨Ø¹Ø¯ Ø§Ø² Reranking) Ø¨Ù‡ Gemini Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
TOP_K_RERANK = 5      

if not all([GOOGLE_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_KEY, COHERE_API_KEY]):
    st.error("âŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ø§Ù‚Øµ: Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ API Ùˆ Supabase Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ .env ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯.")
    
try:
    # Supabase Client
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
except Exception as e:
    st.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Supabase: {e}")
    sys.exit(1)

# ğŸš¨ Cohere Client
@st.cache_resource
def load_cohere_client():
    try:
        co = cohere.Client(api_key=COHERE_API_KEY)
        st.success("âœ… Cohere Client Initialized.", icon="ğŸ§ ")
        return co
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Cohere. RAG ØºÛŒØ±ÙØ¹Ø§Ù„ Ø´Ø¯: {e}")
        return None

COHERE_CLIENT = load_cohere_client()
RERANKER_ACTIVE = (COHERE_CLIENT is not None)

# ----------------------------------------------------------------------

# ---------------- COHERE EMBEDDING CLIENT ----------------

class CohereEmbedClient:
    """Ú©Ù„Ø§ÛŒÙ†Øª Embedding Ø¨Ø±Ø§ÛŒ Cohere API."""
    
    def __init__(self, cohere_client: cohere.Client):
        self.co = cohere_client
        self.model = EMBED_MODEL
        self.dim = EMBED_DIM 

    def embed(self, text: str) -> List[float]:
        if not self.co:
            return [0.0] * self.dim
            
        try:
            response = self.co.embed(
                texts=[text],
                model=self.model,
                input_type="search_query"
            )
            vector = response.embeddings[0]
            
            if len(vector) != self.dim:
                st.warning(f"âš ï¸ Cohere API returned {len(vector)} dims instead of {self.dim}. Using received dimension.")
                self.dim = len(vector)
                
            return vector
            
        except Exception as e:
            st.error(f"âŒ Cohere Embed API Error: {e}")
            return [0.0] * self.dim

# ---------------- LLM Client (Gemini Generation) ----------------
class GeminiClient:
    """Ú©Ù„Ø§ÛŒÙ†Øª Generation Ø¨Ø±Ø§ÛŒ Gemini API."""
    
    GEMINI_API_BASE_URL = "https://generativelanguage.googleapis.com"
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.model = GEMINI_MODEL 
        self.api_path = f"{self.GEMINI_API_BASE_URL}/v1beta/models/{self.model}:generateContent"
        self.full_url = self.api_path 
        self.headers = {"Content-Type": "application/json"}
        self.client = httpx.Client(timeout=180)

    def generate(self, system_prompt: str, history: List[Dict[str, str]], user_prompt: str) -> str:
        
        api_history = []
        for msg in history:
            role = "user" if msg["role"] == "user" else "model"
            if msg.get("content"):
                api_history.append({"role": role, "parts": [{"text": msg["content"]}]})

        contents = [
            {"role": "user", "parts": [{"text": system_prompt}]},
            {"role": "model", "parts": [{"text": "Ø¨Ø§Ø´Ù‡ØŒ Ø¯Ø±Ú© Ø´Ø¯."}]}
        ]
        
        contents.extend(api_history[1:]) 
        contents.append({"role": "user", "parts": [{"text": user_prompt}]})
        
        payload = {"contents": contents}
        params = {"key": self.api_key} 

        try:
            r = self.client.post(self.full_url, headers=self.headers, json=payload, params=params)
            
            if not r.is_success:
                msg = r.json().get("error", {}).get("message", r.text)
                st.error(f"Gemini API Error {r.status_code}: {msg}")
                return "Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ù‚Ø±Ø§Ø±ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ù…Ø¯Ù„ Gemini. (Ù„Ø·ÙØ§Ù‹ GOOGLE_API_KEY Ø±Ø§ Ú†Ú© Ú©Ù†ÛŒØ¯)"
                
            data = r.json()
            return data.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "") or "No response."
        except Exception as e:
            st.error(f"LLM Connection Error: {e}")
            return "Ø®Ø·Ø§ÛŒ Ø´Ø¨Ú©Ù‡ ÛŒØ§ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Gemini."

# ---------------- COHERE RERANKING FUNCTION ----------------

def rerank_documents(query: str, retrieved_chunks: List[Dict[str, Any]], top_k: int) -> List[Dict[str, Any]]:
    """Ø§Ø¹Ù…Ø§Ù„ Ø¨Ø§Ø²Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Cohere Rerank API Ùˆ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ù…Ø±Ù‡ Ø±ÛŒØ±Ù†Ú©."""
    
    if not RERANKER_ACTIVE or not retrieved_chunks:
        # Ø¯Ø± ØµÙˆØ±Øª ØºÛŒØ±ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù†ØŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Similarity Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø±ØªØ¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        retrieved_chunks.sort(key=lambda x: x.get('similarity', 0.0), reverse=True)
        return retrieved_chunks[:top_k]

    try:
        documents_text = [chunk['content'] for chunk in retrieved_chunks]
        
        response = COHERE_CLIENT.rerank(
            model=RERANK_MODEL,
            query=query,
            documents=documents_text,
            top_n=top_k
        )
        
        final_ranked_chunks = []
        chunk_map = {i: chunk for i, chunk in enumerate(retrieved_chunks)}
        
        for rank_result in response.results:
            original_index = rank_result.index
            chunk = chunk_map[original_index]
            chunk['rerank_score'] = rank_result.relevance_score
            final_ranked_chunks.append(chunk)

        # ğŸš¨ ØªØ¶Ù…ÛŒÙ† Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ: Ù…Ø±ØªØ¨Ø·â€ŒØªØ±ÛŒÙ† Ú†Ø§Ù†Ú© (Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ù†Ù…Ø±Ù‡) Ø¯Ø± Ú†Ø§Ù†Ú© 1 Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.
        final_ranked_chunks.sort(key=lambda x: x['rerank_score'], reverse=True)
        
        st.info(f"âœ… Cohere Reranking completed using **{RERANK_MODEL}**. Selecting top {top_k}.", icon="âœ¨")
        
        return final_ranked_chunks
        
    except Exception as e:
        st.error(f"âŒ Cohere Rerank API Error. Falling back to Similarity: {e}")
        retrieved_chunks.sort(key=lambda x: x.get('similarity', 0.0), reverse=True)
        return retrieved_chunks[:top_k]


# ---------------- RAG LOGIC ----------------

@st.cache_resource
def get_clients():
    """Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ©Ø¨Ø§Ø±Û€ Ú©Ù„Ø§ÛŒÙ†Øªâ€ŒÙ‡Ø§ Ùˆ Ú©Ø´ Ú©Ø±Ø¯Ù† Ø¢Ù†Ù‡Ø§."""
    # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø¢ÛŒØ§ COHERE_CLIENT Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±
    if COHERE_CLIENT is None:
        st.error("âŒ Cohere Client Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯. RAG ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª.")
        # Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù„Ø§ÛŒÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§
        return CohereEmbedClient(None), GeminiClient(GOOGLE_API_KEY)
        
    return CohereEmbedClient(COHERE_CLIENT), GeminiClient(GOOGLE_API_KEY)

cohere_embed_service, gemini_service = get_clients()


def retrieve_rag_context(query: str) -> str:
    """Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø²Ù…ÛŒÙ†Ù‡ RAG Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ù†Ú©Ø±ÙˆÙ† Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ùˆ Reranking Ø±Ø§ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    
    match_count = TOP_K_RETRIEVAL 
    
    try:
        # 1. ØªÙˆÙ„ÛŒØ¯ Embedding Ú©ÙˆØ¦Ø±ÛŒ (1024 Ø¨ÙØ¹Ø¯)
        qvec = cohere_embed_service.embed(query)
        
        current_dim = cohere_embed_service.dim 
        if not qvec or len(qvec) != current_dim or all(e == 0.0 for e in qvec):
            return f"RAG disabled: Could not generate a valid {current_dim}-dimension embedding vector using Cohere."

        # 2. Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯Ø± Supabase
        res = supabase.rpc(
            "match_site_pages",
            {"query_embedding": qvec, "match_count": match_count}
        ).execute()

        if not res.data:
            return "No relevant documentation found."
        
        initial_chunks = res.data
        
        # 3. ğŸš¨ Ø§Ø¹Ù…Ø§Ù„ RERANKING (5 Ú†Ø§Ù†Ú©)
        final_ranked_chunks = rerank_documents(query, initial_chunks, TOP_K_RERANK) 
        
        # 4. Ø³Ø§Ø®Øª Context Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø¨Ø§Ø²Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡
        chunks = []
        for i, row in enumerate(final_ranked_chunks):
            similarity = row.get("similarity", "N/A")
            rerank_score = row.get("rerank_score", "N/A")
            
            source_info = row.get("url", "local").split('//')[-1]
            title = row.get("title", "Untitled")
            content = row.get("content", "")
            
            # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…Ø±Ø§Øª Ø¨Ø§ Ø¯Ù‚Øª 3 Ø±Ù‚Ù… Ø§Ø¹Ø´Ø§Ø±
            score_info = f"Sim: {similarity:.3f}, Rerank: {rerank_score:.3f}" if rerank_score != "N/A" else f"Sim: {similarity:.3f}"
            
            # Ú†Ø§Ù†Ú© 1 Ù…Ø±ØªØ¨Ø·â€ŒØªØ±ÛŒÙ† Ú†Ø§Ù†Ú© Ø§Ø³Øª.
            chunks.append(f"--- Chunk {i+1} (Title: {title}, Source: {source_info}, Scores: {score_info}) ---\n{content}")
            
        return "\n\n".join(chunks)
        
    except Exception as e:
        return f"RAG error: {e}"


def generate_rag_response(user_query: str, history: List[Dict[str, str]], context: str) -> str:
    # ... (Ú©Ø¯ Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯) ...
    is_rag_active_and_valid = not context.startswith("RAG disabled:") and not context.startswith("RAG error:")
    is_context_useful = is_rag_active_and_valid and len(context.strip()) > 50

    base_sys_prompt = (
        "Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± RAG Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù…Ø³Ù„Ø· Ø§Ø³Øª. "
        "Ù‡Ù…ÛŒØ´Ù‡ Ù…Ø¤Ø¯Ø¨ Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯. "
        "Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ù‡ **ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡** ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ Ø¨ØªÙˆØ§Ù†ÛŒØ¯ Ø³Ø¤Ø§Ù„Ø§Øª Ø¨Ø¹Ø¯ÛŒ (Ù…Ø§Ù†Ø§Ù†Ù†Ø¯ 'Ø­Ø§Ù„Ø§ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø¢Ù† Ø¨Ú¯Ùˆ') Ø±Ø§ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯. "
    )

    if is_context_useful:
        sys_prompt = (
            f"{base_sys_prompt} Ù‡Ø¯Ù Ø§ØµÙ„ÛŒ Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ **ØªÙ†Ù‡Ø§** 'RAG CONTEXT' Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª. "
            "Ø´Ù…Ø§ **Ø¨Ø§ÛŒØ¯** Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ Ø¹Ø¨Ø§Ø±Øª: 'Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ØŒ' Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒØ¯ Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¯Ø± Context Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ÛŒØ¯. "
            "Ø§Ú¯Ø± Context Ù¾Ø§Ø³Ø® Ø±Ø§ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ÙˆØ¶ÙˆØ­ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯: 'Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ØŒ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ Ù…Ù† Ø´Ø§Ù…Ù„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø§ÛŒÙ† Ø³Ø¤Ø§Ù„ Ù†ÛŒØ³Øª.' "
            "Ø§Ø² Ø¯Ø§Ù†Ø´ Ø¹Ù…ÙˆÙ…ÛŒ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯ Ù…Ú¯Ø± Ø§ÛŒÙ†Ú©Ù‡ Context Ø¨Ù‡ ÙˆØ¶ÙˆØ­ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ø§Ù…Ø±ØªØ¨ØªÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡Ø¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø¨Ø§Ø´Ø¯."
            "\n\n--- RAG CONTEXT ---\n"
            f"{context}\n---"
        )
    else:
        sys_prompt = (
            f"{base_sys_prompt} 'RAG CONTEXT' Ú©Ø§ÙÛŒ ÛŒØ§ Ù…Ø±ØªØ¨Ø·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. "
            "Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **Ø¯Ø§Ù†Ø´ Ø¹Ù…ÙˆÙ…ÛŒ** Ùˆ **ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡** Ø®ÙˆØ¯ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯. "
            "Ú†ÙˆÙ† RAG ÙØ¹Ø§Ù„ Ù†ÛŒØ³ØªØŒ **Ù‡ÛŒÚ† Ø§Ø´Ø§Ø±Ù‡â€ŒØ§ÛŒ** Ø¨Ù‡ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ (Ù…Ø§Ù†Ù†Ø¯ 'Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´') ÛŒØ§ Context Ø¯Ø± Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ù†Ú©Ù†ÛŒØ¯. "
            "ÙÙ‚Ø· Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯."
        )

    response_text = gemini_service.generate(sys_prompt, history, user_query)
    
    return response_text


# Ø§ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ø­Ø§ÙØ¸Ù‡ 
def main_streamlit_app():
    
    st.title(" ğŸ“š Cohere RAG Chatbot ")
    
    st.info(f"ğŸ’¡ **ÙˆØ¶Ø¹ÛŒØª:** Ø­Ø§ÙØ¸Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡â€ŒØ§ÛŒ ÙØ¹Ø§Ù„ØŒ **Embedding:** {EMBED_MODEL} ({cohere_embed_service.dim} Dim), **Reranking:** {RERANK_MODEL} ({TOP_K_RETRIEVAL} -> **{TOP_K_RERANK}**).", icon="ğŸ§ ")

    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "Ø³Ù„Ø§Ù…ØŒ Ù…Ù† ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± RAG Ù‡Ø³ØªÙ…. Ù„Ø·ÙØ§Ù‹ Ø³Ø¤Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯.", "timestamp": datetime.now().isoformat()} 
        ]
    
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            
            if "timestamp" in msg:
                try:
                    dt_obj = datetime.fromisoformat(msg["timestamp"])
                    time_str = dt_obj.strftime("%Y/%m/%d - %H:%M:%S")
                    alignment = "right" if msg["role"] == "user" else "left"
                    st.markdown(f'<p style="color: grey; font-size: small; text-align: {alignment}; margin-bottom: 0px;">{time_str}</p>', unsafe_allow_html=True)
                except ValueError:
                    st.markdown('<p style="color: red; font-size: small;">Ø²Ù…Ø§Ù† Ù†Ø§Ù…Ø¹ØªØ¨Ø±</p>', unsafe_allow_html=True)


    if prompt := st.chat_input("Ø³Ø¤Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯..."):
        current_time = datetime.now().isoformat()
        
        st.session_state.messages.append({"role": "user", "content": prompt, "timestamp": current_time})
        with st.chat_message("user"):
            st.markdown(prompt)
            dt_obj = datetime.fromisoformat(current_time)
            time_str = dt_obj.strftime("%Y/%m/%d - %H:%M:%S")
            st.markdown(f'<p style="color: grey; font-size: small; text-align: right; margin-bottom: 0px;">{time_str}</p>', unsafe_allow_html=True)


        with st.chat_message("assistant"):
            response_placeholder = st.empty() 
            
            with st.spinner("â³ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¯Ø§Ù†Ø´ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®..."):
                
                # 1. Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Context (Ø´Ø§Ù…Ù„ 5 Ú†Ø§Ù†Ú© Ù…Ø±ØªØ¨ Ø´Ø¯Ù‡)
                rag_context = retrieve_rag_context(prompt) 
                
                # 2. ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Context Ú©Ø§Ù…Ù„
                response = generate_rag_response(prompt, st.session_state.messages, rag_context)

                response_placeholder.markdown(response)
                
                assistant_time = datetime.now().isoformat()
                
                st.session_state.messages.append({"role": "assistant", "content": response, "timestamp": assistant_time})
                
                dt_obj = datetime.fromisoformat(assistant_time)
                time_str = dt_obj.strftime("%Y/%m/%d - %H:%M:%S")
                st.markdown(f'<p style="color: grey; font-size: small; text-align: left; margin-bottom: 0px;">{time_str}</p>', unsafe_allow_html=True)


                with st.expander("ğŸ“ Ø¬Ø²Ø¦ÛŒØ§Øª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´"):
                    
                    is_rag_active_and_valid = not rag_context.startswith("RAG disabled:") and not rag_context.startswith("RAG error:")
                    
                    st.markdown(f"**Ù…Ø¯Ù„ LLM:** `{GEMINI_MODEL}`")
                    st.markdown(f"**Ù…Ø¯Ù„ Embedding:** `{EMBED_MODEL}` (Dim: {cohere_embed_service.dim})")
                    st.markdown(f"**Ù…Ø¯Ù„ Reranker:** `{RERANK_MODEL}`")
                    st.markdown(f"**Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§ÙˆÙ„ÛŒÙ‡:** {TOP_K_RETRIEVAL} Ú†Ø§Ù†Ú© | **Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ (Rerank):** **{TOP_K_RERANK}** Ú†Ø§Ù†Ú©")
                    st.markdown("---")
                    
                    if "**Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ØŒ**" in response and is_rag_active_and_valid:
                        st.info("âœ… **ÙˆØ¶Ø¹ÛŒØª Agent:** Agent Ø§Ø² Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯Ù‡ Ùˆ Ù¾Ø§Ø³Ø® Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.", icon="ğŸ“š")
                    elif is_rag_active_and_valid and "No relevant documentation found." in rag_context:
                        st.warning("âš ï¸ **ÙˆØ¶Ø¹ÛŒØª Agent:** Context Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯ØŒ Ø§Ù…Ø§ Ù‡ÛŒÚ† Ø³Ù†Ø¯ Ù…Ø±ØªØ¨Ø·ÛŒ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Agent Ø§Ø² Ø¯Ø§Ù†Ø´ Ø¹Ù…ÙˆÙ…ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯.")
                    elif is_rag_active_and_valid:
                        st.warning("âš ï¸ **ÙˆØ¶Ø¹ÛŒØª Agent:** Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯ØŒ Ø§Ù…Ø§ Agent ØªØµÙ…ÛŒÙ… Ú¯Ø±ÙØª Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†Ø¯. (ÛŒØ§ Context Ú©Ø§ÙÛŒ Ù†Ø¨ÙˆØ¯)")
                    else:
                        error_message = rag_context.split(': ', 1)[-1]
                        st.error(f"âŒ **ÙˆØ¶Ø¹ÛŒØª Agent:** Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯. \n\n**Ø¯Ù„ÛŒÙ„:** {error_message}", icon="ğŸš¨")
                    
                    st.markdown("---")
                    st.markdown(f"**ØªØ¹Ø¯Ø§Ø¯ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ (Memory):** `{len(st.session_state.messages)}`")
                    
                    
                    # ğŸš¨ğŸš¨ğŸš¨ Ù…Ù†Ø·Ù‚ Ù†Ù…Ø§ÛŒØ´ ÙÙ‚Ø· Ù…Ø±ØªØ¨Ø·â€ŒØªØ±ÛŒÙ† Ú†Ø§Ù†Ú© (Ú†Ø§Ù†Ú© 1)
                    if rag_context and is_rag_active_and_valid:
                        
                        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø´Ø±ÙˆØ¹ Ú†Ø§Ù†Ú© 1
                        start_index = rag_context.find("--- Chunk 1 ")
                        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù¾Ø§ÛŒØ§Ù† Ú†Ø§Ù†Ú© 1 (Ù‚Ø¨Ù„ Ø§Ø² Ø´Ø±ÙˆØ¹ Ú†Ø§Ù†Ú© 2 ÛŒØ§ Ø§Ù†ØªÙ‡Ø§ÛŒ Ù…ØªÙ†)
                        end_index_2 = rag_context.find("--- Chunk 2 ", start_index)
                        
                        if start_index != -1: # Ø§Ú¯Ø± Ú†Ø§Ù†Ú© 1 Ù¾ÛŒØ¯Ø§ Ø´Ø¯
                            if end_index_2 == -1:
                                # Ø§Ú¯Ø± Context ÙÙ‚Ø· Ø´Ø§Ù…Ù„ 1 Ú†Ø§Ù†Ú© Ø§Ø³Øª
                                display_chunk = rag_context[start_index:].strip()
                            else:
                                # Ø§Ú¯Ø± Ø¨ÛŒØ´ Ø§Ø² 1 Ú†Ø§Ù†Ú© Ø§Ø³ØªØŒ ÙÙ‚Ø· Ú†Ø§Ù†Ú© 1 Ø±Ø§ ØªØ§ Ù‚Ø¨Ù„ Ø§Ø² Ø´Ø±ÙˆØ¹ Ú†Ø§Ù†Ú© 2 Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
                                display_chunk = rag_context[start_index:end_index_2].strip()

                            st.text_area("ğŸ“„ **Ù…Ø±ØªØ¨Ø·â€ŒØªØ±ÛŒÙ† Ù…Ù†Ø¨Ø¹ (Context):**", display_chunk, height=300)
                            
                            # Ù†Ù…Ø§ÛŒØ´ Context Ú©Ø§Ù…Ù„ Ø¯Ø± ÛŒÚ© Ø¨Ø®Ø´ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ (Ú©ÙˆÚ†Ú©ØªØ±)
                            st.text_area("âœ¨ **Context Ú©Ø§Ù…Ù„ (5 Ú†Ø§Ù†Ú©) Ø§Ø±Ø³Ø§Ù„ÛŒ Ø¨Ù‡ Gemini**", rag_context, height=100, help="Ø§ÛŒÙ† Ù…ØªÙ†ÛŒ Ø§Ø³Øª Ú©Ù‡ Gemini Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø´Ù…Ø§ Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.")
                        else:
                            st.text_area("âœ¨ **Context Ú©Ø§Ù…Ù„ (5 Ú†Ø§Ù†Ú©) Ø§Ø±Ø³Ø§Ù„ÛŒ Ø¨Ù‡ Gemini**", rag_context, height=100)
                    else:
                        st.markdown("**Context Ù…Ø±ØªØ¨Ø·ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ÛŒØ§ÙØª Ù†Ø´Ø¯.**")

if __name__ == "__main__":
    cohere_embed_service, gemini_service = get_clients() 
    main_streamlit_app()
