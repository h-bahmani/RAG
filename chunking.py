
import os
import asyncio
import json
import random 
import re 
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timezone
import sys
import httpx 
from docx import Document
# ğŸš¨ Ø§ÙØ²ÙˆØ¯Ù† Table
from docx.table import Table
from docx.text.paragraph import Paragraph
from dotenv import load_dotenv, find_dotenv

# --- Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ ---
try:
    from supabase import create_client, Client
    import cohere 
except ImportError:
    print("ğŸ›‘ Critical: Ensure you have 'pip install supabase cohere python-dotenv python-docx'")
    sys.exit(1)

# ----------------------------------------
# Û±. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ùˆ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ
# ----------------------------------------
load_dotenv(find_dotenv(), override=True)

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

# ğŸ‘‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª COHERE
COHERE_API_KEY = os.environ.get("COHERE_API_KEY", "") 
COHERE_EMBED_MODEL = "embed-multilingual-v3.0" 

# ğŸš¨ Ø§Ø¨Ø¹Ø§Ø¯ Ù†Ù‡Ø§ÛŒÛŒ ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡ Ø¯Ø± Supabase - Ø¨Ø§ÛŒØ¯ 1024 Ø¨Ø§Ø´Ø¯!
FALLBACK_EMBED_DIM = 1024 

# ğŸ’¡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Semantic Chunking
MAX_CHUNK_SIZE = 700 
CHUNK_OVERLAP = 150 

# ----------------------------------------
# Û². Ù†Ù…ÙˆÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù„Ø§ÛŒÙ†Øªâ€ŒÙ‡Ø§ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# ----------------------------------------
if not all([SUPABASE_URL, SUPABASE_SERVICE_KEY, COHERE_API_KEY]): 
    print("ğŸ›‘ Critical: API Keys (COHERE_API_KEY) and Supabase settings must be set.")
    sys.exit(1)
    
try:
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
    print("âœ… Supabase client initialized.")
except Exception as e:
    print(f"âŒ Configuration Error: Could not initialize Supabase client: {e}")
    sys.exit(1)

# ----------------------------------------
# Û³. Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³ LLM (Ø¨Ø§ Ú©Ù„Ø§ÛŒÙ†Øª COHERE Ùˆ Ø§ØµÙ„Ø§Ø­ Ù…ØªØ¯ close)
# ----------------------------------------

@dataclass
class ProcessedChunk:
    url: str
    chunk_number: int
    title: str
    summary: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] 

class CohereEmbedClient:
    """Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ…Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù‡Ù…Ú¯Ø§Ù… Ùˆ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø§ Cohere REST API."""
    
    def __init__(self, api_key: str, embed_model: str, dim: int):
        self.api_key = api_key
        self.embed_model = embed_model
        self.dim = dim
        # ğŸš¨ Ø§ÛŒØ¬Ø§Ø¯ AsyncClient
        self.co = cohere.AsyncClient(api_key=api_key) 
        
    async def embed_content_direct(self, text: str) -> Optional[List[float]]:
        """Ø¯Ø±ÛŒØ§ÙØª Embedding Ø¨Ø§ Cohere."""
        
        MAX_RETRIES = 3 
        base_delay = 5 
        
        for attempt in range(MAX_RETRIES):
            try:
                # ğŸš¨ Cohere Client V1 Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø¯Ø§Ø±Ø¯
                response = await self.co.embed(
                    texts=[text],
                    model=self.embed_model,
                    input_type="search_document" 
                )
                
                vector = response.embeddings[0]
                
                if len(vector) != self.dim:
                    print(f"âŒ Cohere returned {len(vector)} dims, expected {self.dim}. Skipping.")
                    return None
                
                return vector
                    
            except cohere.errors.CohereAPIError as e:
                if e.http_status in [429, 500, 503] and attempt < MAX_RETRIES - 1:
                    delay = base_delay * (2 ** attempt) + random.uniform(0, 1) 
                    print(f" Â  âš ï¸ Cohere API Server Error ({e.http_status}). Retrying in {delay:.2f}s (Attempt {attempt + 1}/{MAX_RETRIES}).")
                    await asyncio.sleep(delay)
                    continue 
                
                print(f"âŒ Cohere API Error {e.http_status}. Details: {e.message}")
                return None
            
            except Exception as e:
                print(f"âŒ General Error during API call: {type(e).__name__}: {e}.")
                return None
        
        return None 
        
    async def close(self):
        """ğŸš¨ Ø±ÙØ¹ Ù…Ø´Ú©Ù„ AttributeError: 'AsyncClient' object has no attribute 'close'"""
        # Cohere V1 (AsyncClient) Ø§Ø² aclose Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        if hasattr(self.co, 'aclose'):
            await self.co.aclose()
        # Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±ØªØŒ Ú©Ø§Ø±ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… ØªØ§ Ø®Ø·Ø§ Ù†Ø¯Ù‡Ø¯.
        elif hasattr(self.co, 'close'):
            # Ø§Ú¯Ø±Ú†Ù‡ AsyncClient Ø§ÛŒÙ† Ø±Ø§ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø§Ù…Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Fallback Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯.
            self.co.close()
        else:
            print("âš ï¸ Warning: Cohere AsyncClient does not have a close/aclose method.")

# Ù†Ù…ÙˆÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ú©Ù„Ø§ÛŒÙ†Øª
cohere_embed_client = CohereEmbedClient( 
    COHERE_API_KEY, 
    COHERE_EMBED_MODEL,
    FALLBACK_EMBED_DIM
)

# ----------------------------------------
# Û´. ØªÙˆØ§Ø¨Ø¹ Ø§Ø¨Ø²Ø§Ø±ÛŒ Ùˆ Ù‡Ø³ØªÙ‡ RAG
# ----------------------------------------

def simple_text_heuristic(chunk: str) -> Dict[str, str]:
    # ... (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±) ...
    lines = [l.strip() for l in chunk.splitlines() if l.strip()]
    
    if lines:
        title = lines[0].lstrip('#*->- ').strip()[:80]
        
        sentences = re.split(r'(?<=[.!?])\s+', " ".join(lines))
        summary = " ".join([s.strip() for s in sentences[:3] if s.strip()])[:300]
        
        if not summary and lines:
            summary = " ".join(lines[:2])[:300]

    else:
        title = "Untitled (Empty Chunk)"
        summary = "No content to summarize."
        
    return {"title": title, "summary": summary.strip()}


def chunk_text_by_sentence(text: str, max_size: int = MAX_CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    # ... (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±) ...
    sentences = re.split(r'(?<=[.?!])\s+', text)
    if not sentences:
        return []

    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        if len(current_chunk) + len(sentence) + 1 > max_size and current_chunk:
            chunks.append(current_chunk.strip())
            
            overlap_sentences = []
            overlap_len = 0
            
            temp_sentences = re.split(r'(?<=[.?!])\s+', current_chunk)
            
            for s in reversed(temp_sentences):
                s = s.strip()
                if not s: continue
                
                if overlap_len + len(s) + 1 <= overlap:
                    overlap_sentences.insert(0, s) 
                    overlap_len += len(s) + 1
                else:
                    break
            
            current_chunk = " ".join(overlap_sentences).strip()
            
            current_chunk = (current_chunk + " " + sentence).strip()
            
        else:
            current_chunk = (current_chunk + " " + sentence).strip()

    if current_chunk:
        chunks.append(current_chunk.strip())
        
    return chunks


async def insert_chunk(chunk: ProcessedChunk):
    
    if not supabase: return None
    
    if chunk.embedding is None or len(chunk.embedding) != FALLBACK_EMBED_DIM: 
        print(f"ğŸ›‘ Skipping insertion for chunk {chunk.chunk_number} (Invalid embedding length: {len(chunk.embedding) if chunk.embedding else 0}).")
        return None
        
    try:
        data = {
            "url": chunk.url,
            "chunk_number": chunk.chunk_number,
            "title": chunk.title,
            "summary": chunk.summary,
            "content": chunk.content,
            "metadata": chunk.metadata,
            "embedding": chunk.embedding
        }
        
        result = supabase.table("site_pages").insert(data).execute()
        
        print(f"âœ… Inserted chunk {chunk.chunk_number} for {chunk.url}")
        return result
    except Exception as e:
        if 'duplicate key value violates unique constraint' in str(e):
             print(f"âš ï¸ Insertion Skipped: Chunk {chunk.chunk_number} for {chunk.url} already exists in DB.")
        else:
             print(f"âŒ Error inserting chunk: {e}") 
        return None

async def process_chunk(chunk: str, chunk_number: int, url: str) -> Optional[ProcessedChunk]:
    
    fallback_data = simple_text_heuristic(chunk)
    title = fallback_data["title"]
    summary = fallback_data["summary"]
    llm_summary_used = False 

    meta_chunk = f"Title: {title}\nSummary: {summary}\n\nCONTENT:\n{chunk}"

    embedding = await cohere_embed_client.embed_content_direct(meta_chunk)
    
    if embedding is None or len(embedding) != FALLBACK_EMBED_DIM:
        print(f"ğŸ›‘ CRITICAL: Embedding failed or had wrong dimension for chunk {chunk_number}. Skipping insertion.")
        return None 
        
    return ProcessedChunk(
        url=url,
        chunk_number=chunk_number,
        title=title,
        summary=summary,
        content=chunk,
        embedding=embedding,
        metadata={
            "source": url,
            "chunk_length": len(chunk),
            "llm_summary_used": llm_summary_used,
            "embedding_model": COHERE_EMBED_MODEL, 
            "retrieved_at": datetime.now(timezone.utc).isoformat()
        }
    )


def extract_table_text(table: Table) -> str:
    """Ø¬Ø¯ÙˆÙ„ Ø±Ø§ Ø¨Ù‡ ÙØ±Ù…Øª Ù…ØªÙ†ÛŒ Ù‚Ø§Ø¨Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    table_data = []
    
    # ğŸš¨ Ø§ÛŒØ¬Ø§Ø¯ Ù‡Ø¯Ø± Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ø¨Ù‡ØªØ± Ø¬Ø¯ÙˆÙ„ (Ù…Ø«Ù„Ø§Ù‹: Table X: ...)
    table_data.append("--- TABLE START ---")
    
    for i, row in enumerate(table.rows):
        row_text = []
        for j, cell in enumerate(row.cells):
            # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ù‡Ø± Ø³Ù„ÙˆÙ„
            cell_content = cell.text.replace('\n', ' ').strip()
            # ÙØ±Ù…Øª: [Column 1: Value] [Column 2: Value]
            row_text.append(f"[C{j+1}: {cell_content}]")
            
        table_data.append(f"Row {i+1}: {' '.join(row_text)}")
        
    table_data.append("--- TABLE END ---")
    return '\n'.join(table_data)

def extract_text_from_docx(file_path: str) -> str:
    """Extracts all text content (including tables) from a .docx file."""
    try:
        document = Document(file_path)
        full_content = []
        
        for element in document.element.body:
            # ğŸš¨ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
            if element.tag.endswith('p'): # Paragraph
                paragraph = Paragraph(element, document)
                text = paragraph.text.strip()
                if text:
                    full_content.append(text)
            
            # ğŸš¨ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ø¯ÙˆÙ„
            elif element.tag.endswith('tbl'): # Table
                table = Table(element, document)
                table_text = extract_table_text(table)
                if table_text:
                    full_content.append('\n' + table_text + '\n')
                    
        return '\n\n'.join(full_content)
    except Exception as e:
        print(f"âŒ Error extracting text from DOCX: {e}")
        return ""


async def process_and_store_document(url: str, content: str):
    
    chunks = chunk_text_by_sentence(content) 
    print(f"Divided document into {len(chunks)} semantic chunks (Max Size: {MAX_CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP}).")
    
    processed_chunks = []
    
    for i, chunk in enumerate(chunks):
        
        if not chunk.strip():
            print(f"âš ï¸ Skipping empty chunk {i}.")
            continue
            
        print(f"\nâš™ï¸ Processing Chunk {i} of {len(chunks)} (Length: {len(chunk)})...")
        
        try:
            processed_chunk = await process_chunk(chunk, i, url)
            
            if processed_chunk is not None:
                processed_chunks.append(processed_chunk)
                
            if processed_chunk is not None and i < len(chunks) - 1:
                delay = 5 + random.uniform(0, 1) 
                print(f"â³ Waiting {delay:.2f} seconds after successful embedding (Rate Limit Avoidance)...")
                await asyncio.sleep(delay)
                
        except Exception as e:
            print(f"âŒ Critical error during chunk processing {i}: {type(e).__name__}: {e}. Skipping chunk.")
            
    
    print("\nStarting SERIAL insertion to Supabase...")
    for chunk in processed_chunks:
        await insert_chunk(chunk)
    


async def process_local_file(file_path: str):
    # ... (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±) ...
    content = ""
    source_name = os.path.basename(file_path)
    
    # ğŸš¨ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø¯ÙˆÙ„ Ø§Ø² docx
    if file_path.lower().endswith('.docx'):
        print(f"ğŸ“„ Processing DOCX: {source_name}")
        content = extract_text_from_docx(file_path)
    elif file_path.lower().endswith(('.txt', '.md')):
          print(f"ğŸ“„ Processing Text/MD: {source_name}")
          try:
              with open(file_path, "r", encoding="utf-8") as f:
                  content = f.read()
          except Exception as e:
              print(f"âŒ Error reading file: {e}")

    content_length = len(content.strip())
    print(f"ğŸ’¡ Extracted content length: {content_length} characters.")
    
    if not content or content_length == 0:
        print(f"ğŸ›‘ Process stopped: Extracted content from {source_name} is empty or extraction failed.")
        return

    source_url = f"local://{source_name}"
    await process_and_store_document(source_url, content)
    
    print(f"\nâœ… Finished RAG pipeline for local file: {file_path}")

# ----------------------------------------
# Ûµ. ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ
# ----------------------------------------
async def main():
    
    # ğŸš¨ Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.
    local_document_file = r"C:\Users\KIMI\Desktop\n8n-local\crawl4AI-agent\doc\New Microsoft Word Document.docx"
    
    if not os.path.exists(local_document_file):
        print(f"âŒ Document file not found at: {local_document_file}")
        return
    
    print(f"ğŸš€ Starting RAG Pipeline | Embed Model: {COHERE_EMBED_MODEL} ({FALLBACK_EMBED_DIM} Dim)")
    print("-----------------------------------------------------------------")
    
    if not COHERE_API_KEY or len(COHERE_API_KEY) < 30:
        print("\nğŸ›‘ FATAL ERROR: COHERE_API_KEY is missing or too short. Check your .env file.")
        return

    try:
        await process_local_file(local_document_file)
        
    except Exception as e:
        print(f"\nâŒ Pipeline failed during execution. **Detailed Error:** {type(e).__name__}: {e}")
        
    finally:
        print("\nğŸ§¹ Closing clients...")
        # ğŸš¨ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ØªØ¯ close Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
        await cohere_embed_client.close()


if __name__ == "__main__":
    
    try:
        # ğŸš¨ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ú©Ù‡ asyncio.run ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        asyncio.run(main())
    except ValueError as e:
        print(f"âŒ Initialization Error: {e}")
